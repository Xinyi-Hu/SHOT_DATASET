<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Agent System for Comprehensive Soccer Understanding</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px 20px;
            background-color: white;
            box-shadow: 0 0 20px rgba(0,0,0,0.1);
        }

        .header {
            text-align: center;
            margin-bottom: 40px;
        }

        .title {
            font-size: 2.5rem;
            font-weight: 600;
            color: #2c3e50;
            margin-bottom: 30px;
            line-height: 1.2;
        }

        .authors {
            font-size: 1.1rem;
            margin-bottom: 20px;
            color: #34495e;
        }

        .authors a {
            color: #3498db;
            text-decoration: none;
            transition: color 0.3s ease;
        }

        .authors a:hover {
            color: #2980b9;
            text-decoration: underline;
        }

        .affiliation {
            font-size: 1rem;
            color: #7f8c8d;
            margin-bottom: 30px;
        }

        .report-type {
            font-size: 2.3rem;
            color: #060100;
            font-weight: 600;
            margin-bottom: 30px;
        }

        .buttons {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-bottom: 50px;
            flex-wrap: wrap;
        }

        .btn {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            padding: 12px 24px;
            background-color: #34495e;
            color: white;
            text-decoration: none;
            border-radius: 25px;
            font-weight: 500;
            transition: all 0.3s ease;
            font-size: 0.95rem;
        }

        .btn:hover {
            background-color: #2c3e50;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.2);
        }

        .btn-paper { background-color: #e74c3c; }
        .btn-paper:hover { background-color: #c0392b; }

        .btn-code { background-color: #2ecc71; }
        .btn-code:hover { background-color: #27ae60; }

        .btn-dataset { background-color: #f39c12; }
        .btn-dataset:hover { background-color: #e67e22; }

        .btn-arxiv { background-color: #9b59b6; }
        .btn-arxiv:hover { background-color: #8e44ad; }

        .overview-section {
            margin-bottom: 50px;
        }

        .overview-images {
            display: grid;
            grid-template-columns: 1fr;
            gap: 30px;
            margin-bottom: 40px;
        }

        .image-container {
            text-align: center;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            transition: transform 0.3s ease;
        }

        .image-container img:hover {
            transform: scale(1.02);
        }

        .image-caption {
            margin-top: 15px;
            font-style: italic;
            color: #010606;
            font-size: 0.95rem;
            line-height: 1.4;
        }

        .section {
            margin-bottom: 40px;
        }

        .section-title {
            font-size: 2rem;
            font-weight: 300;
            color: #2c3e50;
            margin-bottom: 25px;
            text-align: center;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 15px;
        }

        .abstract-content {
            font-size: 1.1rem;
            line-height: 1.8;
            color: #34495e;
            text-align: justify;
            max-width: 1000px;
            margin: 0 auto;
        }

        .highlight {
            font-weight: 600;
            color: #2c3e50;
        }

        .icon {
            width: 16px;
            height: 16px;
            fill: currentColor;
        }

        @media (max-width: 768px) {
            .container {
                padding: 20px 15px;
            }

            .title {
                font-size: 2rem;
            }

            .buttons {
                flex-direction: column;
                align-items: center;
            }

            .btn {
                width: 200px;
                justify-content: center;
            }

            .authors {
                font-size: 1rem;
            }
        }

        .benchmark-image {
            margin-top: 30px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1 class="title">Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset</h1>
            
            <div class="authors">
                <a href="https://openreview.net/profile?id=~Ruixu_Zhang1">Ruixu Zhang</a><sup>1*</sup>, 
                <a href="https://openreview.net/profile?id=~Yuran_Wang1">Yuran Wang</a><sup>1*</sup>, 
                <a href="https://openreview.net/profile?id=~Xinyi_Hu4">Xinyi Hu</a><sup>1*</sup>, 
                <a href="https://openreview.net/profile?id=~Chaoyu_Mai1">Chaoyu Mai</a><sup>1*</sup>, 
                <a href="https://openreview.net/profile?id=~Wenxuan_Liu1">Wenxuan Liu</a><sup>2</sup>, 
                <a href="https://openreview.net/profile?id=~Danni_Xu3">Danni Xu</a><sup>3</sup>, 
                <a href="https://openreview.net/profile?id=~Xian_Zhong1">Xian Zhong</a><sup>4</sup>, 
                <a href="https://wangzwhu.github.io/home/">Zheng Wang</a><sup>1✉</sup>
            </div>
            
            <div class="affiliation">
                <sup>1</sup>School of Computer Science, Wuhan University<br>
                <sup>2</sup>Peking University<br>
                <sup>3</sup>School of Computing, National University of Singapore<br>
                <sup>4</sup>Wuhan University of Technology
            </div>
            
            <div class="report-type">Technical Report</div>
            
            <div class="buttons">
                <a href="#" class="btn btn-code">
                    <svg class="icon" viewBox="0 0 24 24">
                        <path d="https://github.com/Xinyi-Hu/SHOT-Dataset"/>
                    </svg>
                    Code
                </a>
                <a href="#" class="btn btn-dataset">
                    <svg class="icon" viewBox="0 0 24 24">
                        <circle cx="12" cy="6" r="3"/>
                        <path d="M12 2C10.8 2 9.8 2.5 9.2 3.3L7.5 5.5C7.2 6 7.2 6.6 7.5 7.1L8.5 8.5C9.1 9.3 10.1 9.8 11.2 9.8H12.8C13.9 9.8 14.9 9.3 15.5 8.5L16.5 7.1C16.8 6.6 16.8 6 16.5 5.5L14.8 3.3C14.2 2.5 13.2 2 12 2Z"/>
                    </svg>
                    Dataset
                </a>
            </div>
        </div>

        <div class="overview-section">
            <div class="overview-images">
                <div class="image-container">
                    <img src="teaser9.jpg" alt="SoccerAgent System Overview">
                    <div class="image-caption">
                        <strong>Overview.</strong> (a) Group Intention Forecasting task
forecasts the occurrence time of group intentions by observing individual actions and interactions in early time; (b)The SHOT
dataset provides 5 camera views videos and is annotated with 6 multi-individual attributes to describe the multi-level intention,
including the group intention and the individual intention..
                    </div>
                </div>
            </div>
        </div>

        <div class="section">
            <h2 class="section-title">Abstract</h2>
            <div class="abstract-content">
                Intention recognition has traditionally focused on individual intentions, overlooking the complexities of collective intentions in group settings. To address this limitation, we introduce the concept of group intention, which represents shared goals emerging through the actions of multiple individuals, and Group Intention Forecasting (GIF), a novel task that forecasts when group intentions will occur by analyzing individual actions and interactions before the collective goal becomes apparent. To investigate GIF in a specific scenario, we propose <span class="highlight">SHOT</span>, the first large-scale dataset for GIF, consisting of 1,979 basketball video clips captured from 5 camera views and annotated with 6 types of individual attributes. SHOT is designed with 3 key characteristics: <strong>multi-individual information</strong>, <strong>multi-view adaptability</strong>, and <strong>multi-level intention</strong>, making it well-suited for studying emerging group intentions. Furthermore, we introduce <span class="highlight">GIFT</span> (Group Intention ForecasTer), a framework that extracts fine-grained individual features and models evolving group dynamics to forecast intention emergence. Experimental results confirm the effectiveness of SHOT and GIFT, establishing a strong foundation for future research in group intention forecasting.
            </div>
        </div>

        <div class="section benchmark-image">
            <h2 class="section-title">Dataset Pipeline</h2>
            <div class="image-container">
                <img src="pipeline2.jpg" alt="SoccerBench Benchmark">
                <div class="image-caption">
                    <strong>Pipeline Overview.</strong> Collection: videos are sourced from NBA highlights and full-game replays, then compiled into an unlabeled pool. Categorization: clips are classified by camera view and tactical type. Annotation: features are labeled manually or via tracking models. Structure: video annotations are stored in a JSON file with this structure. Review: annotations are reviewed and relabeled as needed.
                </div>
            <div class="image-container">
                <img src="table.png" alt="SoccerBench Benchmark">
                <div class="image-caption">
                    <strong>Comparison.</strong> Comparison of the proposed SHOT dataset with existing datasets. SA: Sports Analysis, II: Individual Intention, GIF: Group Intention Forecasting.
                </div>

            </div>
        </div>

        <div class="section">
        <h2 class="section-title">Method</h2>
        <div class="image-container">
            <img src="method.jpg" alt="Method Overview">
            <div class="image-caption">
                <strong>Method Overview.</strong> GIFT extracts bounding box, pose, gaze, headpose, velocity, and role features from the τ seen frames (τ ∈ {1, 2, ..., T}). The STGCN Encoder models spatial and temporal patterns. The STGCN Decoder forecasts future features, from which the shooting role is identified to determine the frame number.
            </div>
        </div>
        </div>

        <div class="section">
            <h2 class="section-title">Results</h2>
            <div class="image-container">
                <img src="result.png" alt="Experimental Results">
                <div class="image-caption">
                    <strong>Experimental Results.</strong> Quantitative comparison of leading methods on SHOT. Best performances are highlighted in bold.
                </div>
            </div>
        </div>

<div class="section">
    <h2 class="section-title">Video Clip Selection</h2>
    <div class="image-container">
        <img src="selection.png" alt="Video Clip Selection Process">
        <div class="image-caption">
            <strong>Illustration of the video clip selection process using LosslessCut.</strong> Shooting clips are manually trimmed by identifying their start and end time points, then exported for further processing.
        </div>
    </div>
</div>

<div class="section">
    <h2 class="section-title">View Categorization</h2>
    <div class="image-container">
        <img src="views.jpg" alt="View Categorization">
        <div class="image-caption">
            <strong>Illustration of View1--View5.</strong> Views 1, 2, 4, and 5 each span 30°, while View 3 covers the central 60°.
        </div>
    </div>
</div>

<div class="section">
    <h2 class="section-title">Tactic Categorization</h2>
    <div class="image-container">
        <img src="tactic_categorize.png" alt="Tactic Categorization Process">
        <div class="image-caption">
            <strong>Illustration of the tactic categorization process.</strong> Keywords observed in the video are selected and confirmed for saving.
        </div>
    </div>
</div>

<div class="section">
    <h2 class="section-title">Dataset Statistics</h2>
    <div class="image-container" style="margin-bottom: 30px;">
        <img src="teams2.jpg" alt="NBA Teams Distribution">
        <div class="image-caption">
            <strong>Number of videos for home NBA teams.</strong> Colors indicate different NBA divisions, with each box labeled by the team's abbreviation.
        </div>
    </div>
    <div class="image-container">
        <img src="graph7.jpg" alt="Full Tactical Statistics">
        <div class="image-caption">
            <strong>Full version of tactical statistics.</strong> "1" and "M" denote "One" and "Multi", respectively; "P" and "D" represent "Pass" and "Drive"; while "ST", "LY", and "DK" correspond to "Shoot", "Layup", and "Dunk".
        </div>
    </div>
</div>

<div class="section">
    <h2 class="section-title">Video Examples</h2>
    <div class="image-container">
        <img src="visulization.png" alt="Video Examples from SPOT Dataset">
        <div class="image-caption">
            <strong>Additional video examples from SPOT dataset.</strong> Examples are selected from various camera views and tactic combinations. Each video clip is represented by multiple frames, illustrating the shooting progression.
        </div>
    </div>
</div>
    </div>
</body>
</html>
